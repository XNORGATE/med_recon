import os
import glob
import subprocess
from llama_cpp import Llama

# ç›®éŒ„è¨­å®š
input_dir = "input"
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# æ¨¡å‹è·¯å¾‘
model_path = "./models/JSL-MedMNX-7B-SFT-Q4_K_M.gguf"
llm = Llama(
    model_path=model_path,
    n_ctx=8192,
    n_gpu_layers=-1,     # âœ… å°‡å‰ 100 å±¤æ”¾åˆ° GPUï¼ŒåŠ é€Ÿæ¨ç†
    use_mlock=True,       # âœ… é–å®šè¨˜æ†¶é«”é¿å…è¢«æ›å‡ºï¼ˆå¯é¸ï¼‰
    use_mmap=True,        # âœ… ä½¿ç”¨ mmap åŠ å¿«è¼‰å…¥ï¼ˆé è¨­ç‚º Trueï¼‰
    verbose=True          # âœ… é¡¯ç¤ºè©³ç´° logï¼Œå¹«åŠ©ç¢ºèª GPU æ˜¯å¦å•Ÿç”¨
)
print(llm.ctx)
print("ğŸš€ GPU loaded successfully!")

# è™•ç†æ¯ä¸€ä»½ PDF
for pdf_path in glob.glob(os.path.join(input_dir, "*.pdf")):
    base_name = os.path.splitext(os.path.basename(pdf_path))[0]
    sidecar_path = os.path.join(output_dir, base_name + ".txt")

    # åŸ·è¡Œ OCRmyPDF ä¸¦ç”¢ç”Ÿç´”æ–‡å­— sidecar
    temp_output_pdf = os.path.join(output_dir, base_name + "_tmp.pdf")
    subprocess.run([
        "ocrmypdf",
        "--force-ocr",
        "--sidecar", sidecar_path,
        pdf_path,
        temp_output_pdf
    ], check=True)

    # è®€å– OCR çµæœï¼ˆé™åˆ¶é•·åº¦ï¼‰
    with open(sidecar_path, "r", encoding="utf-8") as f:
        ocr_text = f.read()[:8000]

    # prompt è¨­å®šï¼šè¦æ±‚ markdown çµæ§‹ã€åŠ ä¸Šæ¨™é¡Œç¬¦è™Ÿ
    system_prompt = (
        "You are a professional medical assistant. Given an OCR-extracted pathology report, "
        "analyze and summarize the findings below in markdown format using heading syntax (###):\n\n"
        "### Histologic type:\n- (short summary)\n\n"
        "### Histologic grade:\n- (short summary)\n\n"
        "### Primary tumor (pT):\n- (short summary)\n\n"
        "### FINAL DIAGNOSIS:\n- (summary line)\n- (brief 2â€“4 line explanation based on histologic evidence)\n\n"
        "Rules:\n- Only output the markdown (no explanations or extra tags).\n"
        "Please begin the output with ### Histologic type:"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": ocr_text}
    ]

    # åŸ·è¡Œæ¨¡å‹æ¨è«–
    response = llm.create_chat_completion(
        messages=messages,
        max_tokens=1024,
        stop=["<|end_of_turn|>", "</s>"]
    )
    output_text = response["choices"][0]["message"]["content"].strip()

    # å„²å­˜ Markdown çµæœ
    md_path = os.path.join(output_dir, base_name + ".md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(output_text)

    print(f"âœ… Saved clean output to {md_path}")
