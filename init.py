import os
import glob
import subprocess
import sys
import shutil
from pathlib import Path
from llama_cpp import Llama

# ç›®éŒ„è¨­å®š
input_dir = "input/test"  # è«‹è¨­ç‚ºä½ çš„ä¾†æºè³‡æ–™å¤¾
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# ç¢ºèª Ghostscript å¯åŸ·è¡Œæª”
GS_CMD = shutil.which("gs") or shutil.which("gswin64c") or shutil.which("gswin32c")
if GS_CMD is None:
    print("Ghostscript å¯åŸ·è¡Œæª” gs/gswin64c æœªæ‰¾åˆ°ï¼Œè«‹å®‰è£ä¸¦åŠ å…¥ PATHã€‚")
    sys.exit(1)

# æ¨¡å‹è¨­å®š
model_path = "./models/JSL-MedMNX-7B-SFT-Q4_K_M.gguf"
llm = Llama(
    model_path=model_path,
    n_ctx=8192,
    n_gpu_layers=-1,     # åŠ é€Ÿæ¨ç†
    use_mlock=True,      # é–å®šè¨˜æ†¶é«”é¿å…æ›å‡º
    use_mmap=True,       # mmap åŠ å¿«è¼‰å…¥
    verbose=True         # è©³ç´° log
)
print(llm.ctx)
print("ğŸš€ GPU loaded successfully!")

# è™•ç†æ¯ä¸€ä»½ PDF
for pdf_path in glob.glob(os.path.join(input_dir, "*.pdf")):
    base_name    = os.path.splitext(os.path.basename(pdf_path))[0]
    sidecar_path = os.path.join(output_dir, base_name + ".txt")
    temp_output  = os.path.join(output_dir, base_name + "_tmp.pdf")
    fixed_pdf    = os.path.join(output_dir, base_name + "_fixed.pdf")

    # å˜—è©¦ç›´æ¥ OCR
    try:
        subprocess.run([
            "ocrmypdf",
            "--force-ocr",
            "--sidecar", sidecar_path,
            "--continue-on-soft-render-error",
            pdf_path, temp_output
        ], check=True, capture_output=True, text=True)

    except subprocess.CalledProcessError as e:
        err = e.stderr or ""
        if "NegativeDimensionError" in err:
            print(f"âš ï¸ {base_name} é‡åˆ° NegativeDimensionErrorï¼Œä½¿ç”¨ Ghostscript é‡å¯« PDF â€¦")
            # ç”¨ Ghostscript é‡å¯« PDF
            subprocess.run([
                GS_CMD, "-q", "-o", fixed_pdf,
                "-sDEVICE=pdfwrite",
                "-dSAFER", "-dNOPAUSE", "-dBATCH",
                pdf_path
            ], check=True)
            # å†æ¬¡åŸ·è¡Œ OCR
            subprocess.run([
                "ocrmypdf",
                "--force-ocr",
                "--sidecar", sidecar_path,
                "--continue-on-soft-render-error",
                fixed_pdf, temp_output
            ], check=True)
            os.remove(fixed_pdf)
        else:
            print(f"âŒ {base_name} OCR å¤±æ•—ï¼š{err}")
            continue

    # è®€å– OCR çµæœ
    with open(sidecar_path, "r", encoding="utf-8") as f:
        ocr_text = f.read()[:8000]

    # æº–å‚™ prompt
    system_prompt = (
        "You are a professional medical assistant. Given an OCR-extracted pathology report, "
        "analyze and summarize the findings below in markdown format using heading syntax (###):\n\n"
        "### Histologic type:\n- (short summary)\n\n"
        "### Histologic grade:\n- (short summary)\n\n"
        "### Primary tumor (pT):\n- (short summary)\n\n"
        "### FINAL DIAGNOSIS:\n- (summary line)\n- (brief 2â€“4 line explanation based on histologic evidence)\n\n"
        "Rules:\n- Only output the markdown (no explanations or extra tags).\n"
        "Please begin the output with ### Histologic type:"
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": ocr_text}
    ]

    # æ¨¡å‹æ¨è«–
    response = llm.create_chat_completion(
        messages=messages,
        max_tokens=1024,
        stop=["<|end_of_turn|>", "</s>"]
    )
    output_text = response["choices"][0]["message"]["content"].strip()

    # å„²å­˜çµæœ
    md_path = os.path.join(output_dir, base_name + ".md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(output_text)

    print(f"âœ… Saved clean output to {md_path}")
